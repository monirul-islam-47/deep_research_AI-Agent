import os
from dotenv import load_dotenv

# Import our utility modules
from llm_utils import get_llm_response, analyze_content_prompt, refine_answer_prompt, parse_llm_analysis_response
from search_utils import search_web
from scraper_utils import fetch_and_extract_content # This will use the aliased scraper

# Load environment variables (e.g., for MAX_SEARCH_RESULTS_PER_QUERY if set in .env)
load_dotenv()

# --- Configuration for the Research Agent ---
# MAX_SEARCH_RESULTS_PER_QUERY: How many search results to fetch initially for each query.
# The agent will iterate through these and process the *first* suitable one.
MAX_SEARCH_RESULTS_TO_FETCH = int(os.getenv("MAX_SEARCH_RESULTS_PER_QUERY", 5))

# --- Recursive Research Step Function ---
def conduct_research_step(
    current_query: str,
    current_depth: int,
    max_depth: int,
    visited_urls: set,
    all_research_data: list
):
    """
    Performs one step of the recursive research process.
    - Searches for the current_query.
    - Processes the *first* unvisited, scrapable, and analyzable search result.
    - Stores findings.
    - If a new query is generated by the LLM, recursively calls itself for that new query.

    Args:
        current_query (str): The query for this research step.
        current_depth (int): The current depth of recursion.
        max_depth (int): The maximum allowed recursion depth.
        visited_urls (set): A set of URLs already processed to avoid redundant work.
        all_research_data (list): A list accumulating dictionaries of findings from each step.
    """
    if current_depth > max_depth:
        print(f"‚ÑπÔ∏è Max depth ({max_depth}) reached. Halting research for query: \"{current_query}\"")
        return

    print(f"\n‚û°Ô∏è Depth {current_depth} | Query: \"{current_query}\"")

    search_results = search_web(current_query, max_results=MAX_SEARCH_RESULTS_TO_FETCH)
    if not search_results:
        print(f"‚ö†Ô∏è No search results found for \"{current_query}\". Halting this research path.")
        return

    processed_one_url_successfully_this_step = False
    for search_result in search_results:
        url = search_result.get('href')
        title = search_result.get('title', 'N/A')

        if not url:
            print(f"‚è≠Ô∏è Skipping search result with no URL (Title: {title}).")
            continue
        
        if url in visited_urls:
            print(f"‚è≠Ô∏è Skipping already visited URL: {url}")
            continue
        
        # Mark as visited *before* attempting to process, to handle retries or concurrent scenarios better (though this is serial)
        visited_urls.add(url) 
        
        print(f"üßê Processing URL: {url} (Title: {title})")
        content = fetch_and_extract_content(url) # Uses the chosen scraper from scraper_utils
        
        if content:
            # Successfully scraped content, now analyze with LLM
            print(f"ü§ñ Content scraped. Analyzing with LLM for query: \"{current_query}\"...")

            # Build context from previous summaries (mirrors Go project logic)
            context_parts = [item['summary'] for item in all_research_data if item.get('summary')]
            research_so_far_context = "\n\n---\n\n".join(context_parts) if context_parts else ""

            prompt_for_analysis = analyze_content_prompt(
                current_query=current_query,
                content_from_url=content, # Pass full content
                source_url=url,
                research_so_far_context=research_so_far_context
            )
            llm_analysis_raw = get_llm_response(prompt_for_analysis)

            if llm_analysis_raw:
                summary, new_sub_queries = parse_llm_analysis_response(llm_analysis_raw)
                
                print(f"üìù LLM Summary for {url}: \"{summary[:150].strip()}...\"")
                if new_sub_queries:
                    print(f"üí° LLM Suggested New Queries: {new_sub_queries}")
                else:
                    print(f"üí° LLM suggested no new queries from this content.")

                # Store this step's findings
                all_research_data.append({
                    "depth": current_depth,
                    "query": current_query,
                    "url": url,
                    "title": title,
                    "summary": summary,
                    "generated_queries": new_sub_queries, # Store all for record
                    "raw_content_snippet": content[:250] + "..." # For reference in output file
                })
                
                processed_one_url_successfully_this_step = True

                # If new queries are generated, recurse with the FIRST one (mirrors Go project)
                if new_sub_queries and new_sub_queries[0].strip(): # Check if first query is not empty
                    next_query = new_sub_queries[0].strip()
                    print(f"‚Ü≥ Diving deeper with new query: \"{next_query}\"")
                    conduct_research_step(
                        next_query,
                        current_depth + 1,
                        max_depth,
                        visited_urls,
                        all_research_data
                    )
                else:
                    print(f"‚Ü≥ No valid new query to pursue from this path. Halting this branch.")
                
                # CRITICAL: Processed one URL successfully, break from search_results loop
                # This mirrors the Go project's behavior of processing only one source per step.
                break 
            else:
                print(f"‚ö†Ô∏è LLM analysis failed for content from {url}. Trying next search result if available.")
                # Do not break, allow trying the next search result if LLM fails.
        else:
            print(f"‚ö†Ô∏è Could not extract content from {url}. Trying next search result if available.")
            # Do not break, allow trying the next search result if scraping fails.
    
    if not processed_one_url_successfully_this_step:
        print(f"‚ÑπÔ∏è No processable content found for query \"{current_query}\" at depth {current_depth} after checking available search results. Halting this research path.")

# --- Main Orchestration Function ---
def run_deep_research(initial_query: str, max_depth: int):
    """
    Main function to orchestrate the entire deep research process.

    Args:
        initial_query (str): The starting research query.
        max_depth (int): The maximum depth for the research.

    Returns:
        tuple: (final_answer_string, list_of_all_research_data_dicts)
    """
    visited_urls = set()    # To keep track of URLs we've already processed
    all_research_data = []  # To store data from each step (dict per step)

    # Start the recursive research process
    conduct_research_step(
        current_query=initial_query,
        current_depth=1, # Start at depth 1
        max_depth=max_depth,
        visited_urls=visited_urls,
        all_research_data=all_research_data
    )

    # After all research steps are done, synthesize the final answer
    if not all_research_data:
        print("üòî No research data was gathered. Cannot generate final answer.")
        return "No research data was collected, so no final answer could be synthesized.", all_research_data

    print("\nüèÅ Research phase complete. Synthesizing Final Answer from all findings...")
    
    # Build context for final refinement (concatenation of all summaries)
    final_context_parts = [item['summary'] for item in all_research_data if item.get('summary')]
    if not final_context_parts:
        print("‚ö†Ô∏è No summaries collected during research. Final answer will be based on the initial query only or might be very generic.")
        final_research_context = "No specific summaries were extracted during the research process."
    else:
        final_research_context = "\n\n---\n\n".join(final_context_parts)

    synthesis_prompt = refine_answer_prompt(initial_query, final_research_context)
    final_answer = get_llm_response(synthesis_prompt, system_message="You are an AI research synthesizer tasked with creating a comprehensive answer.")
    
    if not final_answer:
        final_answer = "The LLM failed to generate a final synthesized answer based on the collected research."
        print(f"‚ö†Ô∏è {final_answer}")
    else:
        print("‚úÖ Final answer synthesized.")

    return final_answer, all_research_data

if __name__ == '__main__':
    # Example usage for testing this module directly
    # This requires OPENAI_API_KEY to be set in .env
    print("--- Testing research_agent.py ---")
    if not os.getenv("OPENAI_API_KEY"):
        print("üö´ OPENAI_API_KEY not set. Skipping live test of research_agent.py.")
    else:
        test_query = "What are the main challenges in adopting renewable energy sources?"
        test_max_depth = 1 # Keep depth low for testing to avoid many API calls
        
        print(f"\nüöÄ Running test research for: \"{test_query}\" with max_depth={test_max_depth}")
        
        final_ans, research_summary = run_deep_research(test_query, test_max_depth)
        
        print("\n\n--- TEST RUN COMPLETE ---")
        print("\nFINAL SYNTHESIZED ANSWER:")
        print(final_ans)
        
        print("\n\nDETAILED RESEARCH STEPS RECORDED:")
        if research_summary:
            for i, step_data in enumerate(research_summary):
                print(f"\n  Step {i+1} (Depth {step_data['depth']}):")
                print(f"    Query: \"{step_data['query']}\"")
                print(f"    URL: {step_data['url']}")
                print(f"    Title: \"{step_data['title']}\"")
                print(f"    Summary: \"{step_data['summary'][:100].strip()}...\"")
                if step_data.get('generated_queries'):
                    print(f"    Generated Queries: {step_data['generated_queries']}")
        else:
            print("  No detailed research steps were recorded.")